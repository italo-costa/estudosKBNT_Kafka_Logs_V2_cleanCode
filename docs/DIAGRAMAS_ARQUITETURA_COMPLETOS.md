# ğŸ—ï¸ Diagramas de Arquitetura Completos

Este documento contÃ©m todos os diagramas detalhados da arquitetura do projeto estudosKBNT_Kafka_Logs usando Mermaid.

## ğŸ“Š **1. Arquitetura Geral com Deployment Kubernetes**

```mermaid
graph TB
    subgraph "ğŸŒ External World"
        CLIENT[ğŸ‘¤ Client Applications<br/>Web/Mobile/APIs]
        EXT_API[ğŸŒ External REST API<br/>https://api.external.com<br/>ğŸ”„ Third-party Integration]
        PROMETHEUS[ğŸ“Š Prometheus<br/>Monitoring & Alerting<br/>Port: 9090]
    end
    
    subgraph "â˜¸ï¸ Kubernetes Cluster"
        subgraph "ğŸ“¦ Namespace: kafka"
            subgraph "ğŸ—ï¸ Producer Deployment"
                PROD_POD1[ğŸš€ log-producer-service-0<br/>ğŸ“¡ Spring Boot 3.2<br/>ğŸ›ï¸ Hexagonal Architecture<br/>Port: 8081<br/>CPU: 250m | Memory: 512Mi]
                PROD_POD2[ğŸš€ log-producer-service-1<br/>ğŸ“¡ Spring Boot 3.2<br/>ğŸ›ï¸ Hexagonal Architecture<br/>Port: 8081<br/>CPU: 250m | Memory: 512Mi]
                PROD_POD3[ğŸš€ log-producer-service-2<br/>ğŸ“¡ Spring Boot 3.2<br/>ğŸ›ï¸ Hexagonal Architecture<br/>Port: 8081<br/>CPU: 250m | Memory: 512Mi]
            end
            
            subgraph "ğŸ”¥ AMQ Streams Kafka Cluster"
                subgraph "ğŸ“¨ Kafka Topics"
                    T1[ğŸ“‹ application-logs<br/>Partitions: 3<br/>Replication: 2<br/>ğŸ”„ General application logs]
                    T2[ğŸš¨ error-logs<br/>Partitions: 3<br/>Replication: 2<br/>âŒ Error & Fatal logs]
                    T3[ğŸ” audit-logs<br/>Partitions: 2<br/>Replication: 2<br/>ğŸ›¡ï¸ Security & Auth logs]
                    T4[ğŸ’° financial-logs<br/>Partitions: 3<br/>Replication: 2<br/>ğŸ’³ Transaction logs]
                end
                
                subgraph "âš–ï¸ Kafka Brokers"
                    KAFKA1[kafka-cluster-kafka-0<br/>Port: 9092<br/>CPU: 500m | Memory: 1Gi]
                    KAFKA2[kafka-cluster-kafka-1<br/>Port: 9092<br/>CPU: 500m | Memory: 1Gi]
                    KAFKA3[kafka-cluster-kafka-2<br/>Port: 9092<br/>CPU: 500m | Memory: 1Gi]
                end
            end
            
            subgraph "ğŸ”„ Consumer Deployment"
                CONS_POD1[ğŸ”„ log-consumer-service-0<br/>ğŸ“¥ Spring Boot 3.2<br/>ğŸŒ API Integration<br/>Port: 8082<br/>CPU: 250m | Memory: 512Mi]
                CONS_POD2[ğŸ”„ log-consumer-service-1<br/>ğŸ“¥ Spring Boot 3.2<br/>ğŸŒ API Integration<br/>Port: 8082<br/>CPU: 250m | Memory: 512Mi]
            end
            
            subgraph "ğŸ˜ Zookeeper Cluster"
                ZK1[zookeeper-0<br/>Port: 2181<br/>CPU: 250m | Memory: 512Mi]
                ZK2[zookeeper-1<br/>Port: 2181<br/>CPU: 250m | Memory: 512Mi]
                ZK3[zookeeper-2<br/>Port: 2181<br/>CPU: 250m | Memory: 512Mi]
            end
        end
        
        subgraph "ğŸ“¦ Namespace: monitoring"
            PROM_POD[ğŸ“Š Prometheus Server<br/>Metrics Collection<br/>Port: 9090<br/>Storage: 10Gi]
            GRAF_POD[ğŸ“ˆ Grafana Dashboard<br/>Visualization<br/>Port: 3000]
        end
    end
    
    %% Client Interactions
    CLIENT -->|ğŸ“¡ HTTP POST /api/v1/logs<br/>âš¡ SYNCHRONOUS| PROD_POD1
    CLIENT -->|ğŸ“¡ HTTP POST /api/v1/logs<br/>âš¡ SYNCHRONOUS| PROD_POD2
    CLIENT -->|ğŸ“¡ HTTP POST /api/v1/logs<br/>âš¡ SYNCHRONOUS| PROD_POD3
    
    %% Producer to Kafka (Async)
    PROD_POD1 -->|ğŸ“¤ Publish Messages<br/>ğŸ”„ ASYNCHRONOUS| T1
    PROD_POD1 -->|ğŸ“¤ Publish Messages<br/>ğŸ”„ ASYNCHRONOUS| T2
    PROD_POD2 -->|ğŸ“¤ Publish Messages<br/>ğŸ”„ ASYNCHRONOUS| T3
    PROD_POD3 -->|ğŸ“¤ Publish Messages<br/>ğŸ”„ ASYNCHRONOUS| T4
    
    %% Kafka to Consumer (Async)
    T1 -->|ğŸ“¥ Consume Messages<br/>ğŸ”„ ASYNCHRONOUS| CONS_POD1
    T2 -->|ğŸ“¥ Consume Messages<br/>ğŸ”„ ASYNCHRONOUS| CONS_POD1
    T3 -->|ğŸ“¥ Consume Messages<br/>ğŸ”„ ASYNCHRONOUS| CONS_POD2
    T4 -->|ğŸ“¥ Consume Messages<br/>ğŸ”„ ASYNCHRONOUS| CONS_POD2
    
    %% Consumer to External API (Sync)
    CONS_POD1 -->|ğŸŒ REST Calls<br/>âš¡ SYNCHRONOUS| EXT_API
    CONS_POD2 -->|ğŸŒ REST Calls<br/>âš¡ SYNCHRONOUS| EXT_API
    
    %% Metrics
    PROD_POD1 -->|ğŸ“Š Metrics Export<br/>ğŸ”„ ASYNCHRONOUS| PROMETHEUS
    PROD_POD2 -->|ğŸ“Š Metrics Export<br/>ğŸ”„ ASYNCHRONOUS| PROMETHEUS
    CONS_POD1 -->|ğŸ“Š Metrics Export<br/>ğŸ”„ ASYNCHRONOUS| PROMETHEUS
    CONS_POD2 -->|ğŸ“Š Metrics Export<br/>ğŸ”„ ASYNCHRONOUS| PROMETHEUS
    PROMETHEUS -->|ğŸ“ˆ Data Visualization| GRAF_POD
    
    %% Kafka Dependencies
    KAFKA1 -.->|Cluster Coordination| ZK1
    KAFKA2 -.->|Cluster Coordination| ZK2
    KAFKA3 -.->|Cluster Coordination| ZK3
    
    %% Styling
    classDef prodPod fill:#4ecdc4,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef consPod fill:#45b7d1,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef kafkaTopic fill:#ff6b6b,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef kafkaBroker fill:#feca57,stroke:#2c3e50,stroke-width:2px,color:#000
    classDef external fill:#96ceb4,stroke:#2c3e50,stroke-width:2px,color:#000
    classDef zookeeper fill:#a55eea,stroke:#2c3e50,stroke-width:2px,color:#fff
    
    class PROD_POD1,PROD_POD2,PROD_POD3 prodPod
    class CONS_POD1,CONS_POD2 consPod
    class T1,T2,T3,T4 kafkaTopic
    class KAFKA1,KAFKA2,KAFKA3 kafkaBroker
    class CLIENT,EXT_API,PROMETHEUS external
    class ZK1,ZK2,ZK3 zookeeper
```

---

## ğŸ”„ **2. Sequence Diagram - Fluxo Completo de Processamento**

```mermaid
sequenceDiagram
    participant C as ğŸ‘¤ Client App
    participant LB as âš–ï¸ K8s LoadBalancer
    participant P as ğŸš€ Log Producer<br/>(Pod: log-producer-0)
    participant V as âœ… Validation Service<br/>(Domain Layer)
    participant R as ğŸ”„ Routing Service<br/>(Domain Layer)
    participant KP as ğŸ“¤ Kafka Publisher<br/>(Infrastructure)
    participant T as ğŸ“¨ Kafka Topic<br/>(application-logs)
    participant LC as ğŸ“¥ Log Consumer<br/>(Pod: log-consumer-0)
    participant EA as ğŸŒ External API<br/>(https://api.external.com)
    participant M as ğŸ“Š Metrics<br/>(Prometheus)
    
    Note over C,M: ğŸ”„ Complete Log Processing Flow
    
    rect rgb(240, 248, 255)
        Note over C,P: Phase 1: Synchronous HTTP Request
        C->>+LB: POST /api/v1/logs<br/>âš¡ SYNC HTTP Request<br/>Content-Type: application/json
        LB->>+P: Forward to available pod<br/>âš¡ SYNC (Load Balanced)
        
        Note over P,R: Phase 2: Domain Processing (Hexagonal Architecture)
        P->>+V: validateLogEntry()<br/>âš¡ SYNC Domain Validation
        V-->>-P: ValidationResult<br/>âœ… Valid/Invalid + Errors
        
        alt Log is valid
            P->>+R: determineKafkaTopic()<br/>âš¡ SYNC Routing Logic
            R-->>-P: topic="application-logs"<br/>ğŸ¯ Smart Routing Result
        else Log is invalid
            P-->>LB: 400 Bad Request<br/>âŒ Validation Errors
            LB-->>-C: 400 Bad Request
        end
    end
    
    rect rgb(245, 255, 245)
        Note over P,T: Phase 3: Asynchronous Message Publishing
        P->>+KP: publishLog(logEntry, topic)<br/>ğŸ”„ ASYNC Publishing
        KP->>T: Send Message to Topic<br/>ğŸ”„ ASYNC Kafka Publish<br/>Partition: auto-assigned
        T-->>KP: Acknowledgment<br/>âœ… Message Stored (offset: 12345)
        KP-->>-P: PublishResult<br/>âœ… Success
        
        P->>M: Increment published_logs_total<br/>ğŸ“Š Metrics (ASYNC)
        P-->>LB: 200 OK<br/>âœ… Success Response
        LB-->>-C: 200 OK<br/>âœ… Log Accepted
    end
    
    rect rgb(255, 245, 238)
        Note over T,EA: Phase 4: Asynchronous Message Consumption & External Integration
        T->>+LC: Poll & Consume Message<br/>ğŸ”„ ASYNC Kafka Consumer<br/>Consumer Group: log-consumer-group
        
        Note over LC: Process Message<br/>ğŸ”„ Business Logic
        
        LC->>+EA: POST /webhook/logs<br/>âš¡ SYNC REST API Call<br/>Content-Type: application/json<br/>Timeout: 30s
        
        alt External API Success
            EA-->>-LC: 200 OK<br/>âœ… Processing Success<br/>Response: {"status": "received"}
            LC->>T: Commit Offset<br/>âœ… Message Processed (offset: 12345)
            LC->>M: Increment processed_logs_total<br/>ğŸ“Š Success Metrics
        else External API Failure
            EA-->>LC: 500 Internal Server Error<br/>âŒ Processing Failed
            LC->>M: Increment api_failures_total<br/>ğŸ“Š Error Metrics
            LC->>T: No Commit<br/>ğŸ”„ Message will be retried
            Note over LC: Retry Logic<br/>â° Exponential Backoff
        end
        
        LC-->>-T: Consumer Processing Complete
    end
    
    rect rgb(248, 248, 255)
        Note over M: Phase 5: Observability & Monitoring
        M->>M: Collect & Aggregate Metrics<br/>ğŸ“Š Prometheus Scraping
        Note over M: Available Metrics:<br/>â€¢ logs_published_total<br/>â€¢ logs_processed_total<br/>â€¢ api_response_time_seconds<br/>â€¢ kafka_consumer_lag
    end
```

---

## ğŸ—ï¸ **3. Hexagonal Architecture - Log Producer Service**

```mermaid
graph TB
    subgraph "ğŸŒ External Actors"
        HTTP[ğŸ“¡ HTTP Clients<br/>Web, Mobile, APIs]
        KAFKA_EXT[ğŸ”¥ Apache Kafka<br/>Message Broker]
        METRICS_EXT[ğŸ“Š Prometheus<br/>Metrics Collector]
    end
    
    subgraph "ğŸ—ï¸ Hexagonal Architecture - Log Producer Service"
        subgraph "ğŸŒ Infrastructure Layer (Adapters)"
            subgraph "ğŸ“¥ Input Adapters (Driving)"
                REST_CTRL[ğŸ“¡ REST Controller<br/>LogController<br/>â€¢ POST /api/v1/logs<br/>â€¢ POST /api/v1/logs/batch<br/>â€¢ GET /actuator/health]
            end
            
            subgraph "ğŸ“¤ Output Adapters (Driven)"
                KAFKA_ADAPTER[ğŸ“¤ Kafka Publisher Adapter<br/>KafkaLogPublisherAdapter<br/>â€¢ Message serialization<br/>â€¢ Topic publishing<br/>â€¢ Error handling]
                METRICS_ADAPTER[ğŸ“Š Metrics Adapter<br/>MicrometerMetricsAdapter<br/>â€¢ Counter increments<br/>â€¢ Timer recordings<br/>â€¢ Gauge updates]
            end
            
            subgraph "âš™ï¸ Configuration"
                KAFKA_CONFIG[âš™ï¸ Kafka Configuration<br/>KafkaConfig<br/>â€¢ Producer settings<br/>â€¢ Serializers<br/>â€¢ Retry policies]
                DOMAIN_CONFIG[âš™ï¸ Domain Configuration<br/>DomainConfig<br/>â€¢ Service beans<br/>â€¢ Dependencies]
            end
        end
        
        subgraph "âš™ï¸ Application Layer (Use Cases)"
            PROD_UC[âš™ï¸ Log Production UseCase<br/>LogProductionUseCaseImpl<br/>â€¢ Orchestrate validation<br/>â€¢ Coordinate routing<br/>â€¢ Handle publishing]
            VALID_UC[âœ… Validation UseCase<br/>LogValidationUseCaseImpl<br/>â€¢ Individual validation<br/>â€¢ Batch validation<br/>â€¢ Result aggregation]
        end
        
        subgraph "ğŸ›ï¸ Domain Layer (Core Business Logic)"
            subgraph "ğŸ“‹ Entities"
                LOG_ENTITY[ğŸ“‹ LogEntry Entity<br/>â€¢ Business methods<br/>â€¢ State management<br/>â€¢ Domain validation]
            end
            
            subgraph "ğŸ’ Value Objects"
                LOG_LEVEL[ğŸ’ LogLevel<br/>DEBUG, INFO, WARN<br/>ERROR, FATAL]
                REQUEST_ID[ğŸ’ RequestId<br/>UUID validation<br/>Immutable]
                SERVICE_NAME[ğŸ’ ServiceName<br/>Format validation<br/>Length constraints]
            end
            
            subgraph "ğŸ¯ Domain Services"
                ROUTING_SVC[ğŸ”„ Log Routing Service<br/>LogRoutingService<br/>â€¢ Smart topic selection<br/>â€¢ Priority determination<br/>â€¢ Partition key generation]
                VALIDATION_SVC[âœ… Validation Service<br/>LogValidationService<br/>â€¢ Business rules<br/>â€¢ Data integrity<br/>â€¢ Duplicate detection]
            end
            
            subgraph "ğŸ”Œ Port Interfaces"
                subgraph "ğŸ“¥ Input Ports"
                    LOG_PROD_PORT[ğŸ“¥ LogProductionUseCase<br/>Interface]
                    LOG_VALID_PORT[ğŸ“¥ LogValidationUseCase<br/>Interface]
                end
                
                subgraph "ğŸ“¤ Output Ports"
                    PUBLISHER_PORT[ğŸ“¤ LogPublisherPort<br/>Interface]
                    METRICS_PORT[ğŸ“Š MetricsPort<br/>Interface]
                end
            end
        end
    end
    
    %% External connections to adapters
    HTTP -->|ğŸ“¡ HTTP Requests<br/>âš¡ SYNC| REST_CTRL
    KAFKA_ADAPTER -->|ğŸ“¤ Publish Messages<br/>ğŸ”„ ASYNC| KAFKA_EXT
    METRICS_ADAPTER -->|ğŸ“Š Export Metrics<br/>ğŸ”„ ASYNC| METRICS_EXT
    
    %% Infrastructure to Application
    REST_CTRL -->|ğŸ“‹ DTO â†’ Domain Models<br/>âš¡ SYNC| PROD_UC
    PROD_UC -->|ğŸ“¤ Domain Models<br/>ğŸ”„ ASYNC| KAFKA_ADAPTER
    PROD_UC -->|ğŸ“Š Metrics Data<br/>ğŸ”„ ASYNC| METRICS_ADAPTER
    
    %% Application to Domain
    PROD_UC -->|ğŸ“‹ LogEntry<br/>âš¡ SYNC| VALIDATION_SVC
    PROD_UC -->|ğŸ“‹ LogEntry<br/>âš¡ SYNC| ROUTING_SVC
    VALID_UC -->|ğŸ“‹ LogEntry<br/>âš¡ SYNC| VALIDATION_SVC
    
    %% Domain interactions
    LOG_ENTITY -.->|Uses| LOG_LEVEL
    LOG_ENTITY -.->|Uses| REQUEST_ID
    LOG_ENTITY -.->|Uses| SERVICE_NAME
    VALIDATION_SVC -.->|Validates| LOG_ENTITY
    ROUTING_SVC -.->|Routes| LOG_ENTITY
    
    %% Port implementations
    PROD_UC -.->|Implements| LOG_PROD_PORT
    VALID_UC -.->|Implements| LOG_VALID_PORT
    KAFKA_ADAPTER -.->|Implements| PUBLISHER_PORT
    METRICS_ADAPTER -.->|Implements| METRICS_PORT
    
    %% Configuration dependencies
    KAFKA_CONFIG -.->|Configures| KAFKA_ADAPTER
    DOMAIN_CONFIG -.->|Configures| VALIDATION_SVC
    DOMAIN_CONFIG -.->|Configures| ROUTING_SVC
    
    %% Styling
    classDef external fill:#96ceb4,stroke:#2c3e50,stroke-width:2px,color:#000
    classDef infrastructure fill:#f39c12,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef application fill:#3498db,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef domain fill:#e74c3c,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef entity fill:#9b59b6,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef valueObject fill:#1abc9c,stroke:#2c3e50,stroke-width:2px,color:#fff
    classDef port fill:#34495e,stroke:#2c3e50,stroke-width:2px,color:#fff
    
    class HTTP,KAFKA_EXT,METRICS_EXT external
    class REST_CTRL,KAFKA_ADAPTER,METRICS_ADAPTER,KAFKA_CONFIG,DOMAIN_CONFIG infrastructure
    class PROD_UC,VALID_UC application
    class ROUTING_SVC,VALIDATION_SVC domain
    class LOG_ENTITY entity
    class LOG_LEVEL,REQUEST_ID,SERVICE_NAME valueObject
    class LOG_PROD_PORT,LOG_VALID_PORT,PUBLISHER_PORT,METRICS_PORT port
```

---

## ğŸŒŠ **4. Data Flow & Topic Routing Strategy**

```mermaid
flowchart TD
    START([ğŸ“¥ Log Entry Received<br/>HTTP POST Request]) --> VALIDATE{âœ… Validation<br/>Service}
    
    VALIDATE -->|âŒ Invalid| REJECT[âŒ HTTP 400<br/>Bad Request<br/>Return validation errors]
    VALIDATE -->|âœ… Valid| ROUTE[ğŸ”„ Routing Service<br/>Determine destination]
    
    ROUTE --> SECURITY_CHECK{ğŸ” Security<br/>Related?}
    ROUTE --> ERROR_CHECK{ğŸš¨ Error<br/>Level?}
    ROUTE --> FINANCIAL_CHECK{ğŸ’° Financial<br/>Transaction?}
    ROUTE --> DEFAULT_ROUTE[ğŸ“‹ Default Routing]
    
    SECURITY_CHECK -->|âœ… Yes<br/>auth, login, security| AUDIT_TOPIC[ğŸ” audit-logs<br/>Partitions: 2<br/>Retention: 30 days<br/>High Security]
    
    ERROR_CHECK -->|âœ… Yes<br/>ERROR or FATAL| ERROR_TOPIC[ğŸš¨ error-logs<br/>Partitions: 3<br/>Retention: 90 days<br/>Priority: HIGH]
    
    FINANCIAL_CHECK -->|âœ… Yes<br/>payment, transaction| FINANCIAL_TOPIC[ğŸ’° financial-logs<br/>Partitions: 3<br/>Retention: 7 years<br/>Compliance: Required]
    
    DEFAULT_ROUTE --> APPLICATION_TOPIC[ğŸ“‹ application-logs<br/>Partitions: 3<br/>Retention: 7 days<br/>General Purpose]
    
    AUDIT_TOPIC --> KAFKA_CLUSTER[ğŸ”¥ Kafka Cluster<br/>3 Brokers<br/>Replication Factor: 2]
    ERROR_TOPIC --> KAFKA_CLUSTER
    FINANCIAL_TOPIC --> KAFKA_CLUSTER
    APPLICATION_TOPIC --> KAFKA_CLUSTER
    
    KAFKA_CLUSTER --> CONSUMER_GROUP[ğŸ‘¥ Consumer Group<br/>log-consumer-group<br/>2 Consumer Instances]
    
    CONSUMER_GROUP --> CONSUMER_1[ğŸ“¥ Consumer Instance 1<br/>Handles: audit-logs, error-logs]
    CONSUMER_GROUP --> CONSUMER_2[ğŸ“¥ Consumer Instance 2<br/>Handles: application-logs, financial-logs]
    
    CONSUMER_1 --> PRIORITY_API[âš¡ High Priority API<br/>Security & Error Endpoint<br/>Timeout: 10s<br/>Retry: 3x]
    CONSUMER_2 --> STANDARD_API[ğŸ”„ Standard API<br/>General Processing Endpoint<br/>Timeout: 30s<br/>Retry: 2x]
    
    PRIORITY_API --> SUCCESS_1{âœ… API<br/>Success?}
    STANDARD_API --> SUCCESS_2{âœ… API<br/>Success?}
    
    SUCCESS_1 -->|âœ… Yes| COMMIT_1[âœ… Commit Offset<br/>Mark as processed]
    SUCCESS_1 -->|âŒ No| RETRY_1[ğŸ”„ Retry Queue<br/>Exponential backoff]
    
    SUCCESS_2 -->|âœ… Yes| COMMIT_2[âœ… Commit Offset<br/>Mark as processed]
    SUCCESS_2 -->|âŒ No| RETRY_2[ğŸ”„ Retry Queue<br/>Exponential backoff]
    
    COMMIT_1 --> METRICS[ğŸ“Š Success Metrics<br/>Prometheus Export]
    COMMIT_2 --> METRICS
    RETRY_1 --> METRICS_ERROR[ğŸ“Š Error Metrics<br/>Failed API calls]
    RETRY_2 --> METRICS_ERROR
    
    %% Styling
    classDef startEnd fill:#2ecc71,stroke:#27ae60,stroke-width:2px,color:#fff
    classDef decision fill:#f39c12,stroke:#d68910,stroke-width:2px,color:#fff
    classDef process fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff
    classDef topic fill:#e74c3c,stroke:#c0392b,stroke-width:2px,color:#fff
    classDef error fill:#e67e22,stroke:#d35400,stroke-width:2px,color:#fff
    classDef success fill:#27ae60,stroke:#229954,stroke-width:2px,color:#fff
    
    class START,REJECT startEnd
    class VALIDATE,SECURITY_CHECK,ERROR_CHECK,FINANCIAL_CHECK,SUCCESS_1,SUCCESS_2 decision
    class ROUTE,CONSUMER_GROUP,CONSUMER_1,CONSUMER_2,PRIORITY_API,STANDARD_API process
    class AUDIT_TOPIC,ERROR_TOPIC,FINANCIAL_TOPIC,APPLICATION_TOPIC topic
    class RETRY_1,RETRY_2,METRICS_ERROR error
    class COMMIT_1,COMMIT_2,METRICS success
```

---

## ğŸ“Š **5. Monitoring & Observability Architecture**

```mermaid
graph TB
    subgraph "ğŸ¯ Application Metrics Sources"
        subgraph "ğŸš€ Log Producer Metrics"
            PROD_METRICS[ğŸ“Š Producer Metrics<br/>â€¢ logs_published_total<br/>â€¢ logs_validation_errors_total<br/>â€¢ logs_publishing_errors_total<br/>â€¢ logs_processing_time_seconds<br/>â€¢ logs_level_count{level}]
        end
        
        subgraph "ğŸ“¥ Log Consumer Metrics"
            CONS_METRICS[ğŸ“Š Consumer Metrics<br/>â€¢ logs_consumed_total<br/>â€¢ logs_processed_total<br/>â€¢ api_calls_total<br/>â€¢ api_failures_total<br/>â€¢ api_response_time_seconds]
        end
        
        subgraph "ğŸ”¥ Kafka Metrics"
            KAFKA_METRICS[ğŸ“Š Kafka Metrics<br/>â€¢ kafka_topic_partitions<br/>â€¢ kafka_consumer_lag<br/>â€¢ kafka_messages_per_sec<br/>â€¢ kafka_broker_availability]
        end
    end
    
    subgraph "ğŸ“Š Monitoring Infrastructure"
        subgraph "ğŸ“¡ Metrics Collection"
            PROMETHEUS[ğŸ“Š Prometheus Server<br/>â€¢ Scraping: /actuator/prometheus<br/>â€¢ Retention: 30 days<br/>â€¢ Scrape interval: 15s<br/>Storage: 10Gi]
        end
        
        subgraph "ğŸ“ˆ Visualization"
            GRAFANA[ğŸ“ˆ Grafana Dashboard<br/>â€¢ Real-time charts<br/>â€¢ Alerting rules<br/>â€¢ Custom dashboards<br/>Port: 3000]
        end
        
        subgraph "ğŸš¨ Alerting"
            ALERTMANAGER[ğŸš¨ Alert Manager<br/>â€¢ Notification routing<br/>â€¢ Alert grouping<br/>â€¢ Silence management]
        end
    end
    
    subgraph "ğŸ“± Notification Channels"
        SLACK[ğŸ’¬ Slack<br/>Channel: #kafka-alerts]
        EMAIL[ğŸ“§ Email<br/>DevOps Team]
        WEBHOOK[ğŸ”— Webhook<br/>Incident Management]
    end
    
    %% Metrics flow
    PROD_METRICS -->|ğŸ“Š HTTP /actuator/prometheus<br/>ğŸ”„ Every 15s| PROMETHEUS
    CONS_METRICS -->|ğŸ“Š HTTP /actuator/prometheus<br/>ğŸ”„ Every 15s| PROMETHEUS
    KAFKA_METRICS -->|ğŸ“Š JMX Metrics<br/>ğŸ”„ Every 30s| PROMETHEUS
    
    %% Visualization
    PROMETHEUS -->|ğŸ“ˆ PromQL Queries<br/>Real-time data| GRAFANA
    PROMETHEUS -->|ğŸš¨ Alert Rules<br/>Threshold monitoring| ALERTMANAGER
    
    %% Alerting
    ALERTMANAGER -->|ğŸ’¬ Critical alerts| SLACK
    ALERTMANAGER -->|ğŸ“§ Daily summaries| EMAIL
    ALERTMANAGER -->|ğŸ”— Incident creation| WEBHOOK
    
    %% Key Metrics Details
    subgraph "ğŸ¯ Key Performance Indicators"
        KPI[ğŸ“Š Critical Metrics<br/>â€¢ Throughput: logs/second<br/>â€¢ Latency: P95 response time<br/>â€¢ Error Rate: % failed requests<br/>â€¢ Availability: % uptime<br/>â€¢ Consumer Lag: messages behind]
    end
    
    PROMETHEUS -.->|ğŸ“Š Aggregated data| KPI
    
    %% Alert Examples
    subgraph "ğŸš¨ Alert Conditions"
        ALERTS[ğŸš¨ Alert Rules<br/>â€¢ Error rate > 5% (5m)<br/>â€¢ Consumer lag > 1000 msgs<br/>â€¢ API response time > 10s<br/>â€¢ Service down > 1min<br/>â€¢ Disk usage > 85%]
    end
    
    ALERTMANAGER -.->|ğŸ”” Configured alerts| ALERTS
    
    classDef metrics fill:#3498db,stroke:#2980b9,stroke-width:2px,color:#fff
    classDef monitoring fill:#e74c3c,stroke:#c0392b,stroke-width:2px,color:#fff
    classDef notification fill:#f39c12,stroke:#d68910,stroke-width:2px,color:#fff
    classDef info fill:#95a5a6,stroke:#7f8c8d,stroke-width:2px,color:#fff
    
    class PROD_METRICS,CONS_METRICS,KAFKA_METRICS metrics
    class PROMETHEUS,GRAFANA,ALERTMANAGER monitoring
    class SLACK,EMAIL,WEBHOOK notification
    class KPI,ALERTS info
```

Agora vou salvar estes diagramas detalhados no repositÃ³rio:
